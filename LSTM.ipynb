{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f41c0f0c",
   "metadata": {},
   "source": [
    "\n",
    "# Develop LSTM = long short term memory network for univariate Time Series Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39b13160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.11.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.11.0\n",
      "  Downloading tensorflow_intel-2.11.0-cp39-cp39-win_amd64.whl (266.3 MB)\n",
      "     -------------------------------------- 266.3/266.3 MB 2.1 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     -------------------------------------- 57.5/57.5 kB 116.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (63.4.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.14.1)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ------------------------------------ 126.5/126.5 kB 573.0 kB/s eta 0:00:00\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Downloading tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "     ---------------------------------------- 6.0/6.0 MB 9.3 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 7.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (21.3)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Downloading keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 5.4 MB/s eta 0:00:00\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Downloading tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "     -------------------------------------- 439.2/439.2 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Downloading protobuf-3.19.6-cp39-cp39-win_amd64.whl (895 kB)\n",
      "     -------------------------------------- 895.9/895.9 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.20 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (1.21.5)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-15.0.6.1-py2.py3-none-win_amd64.whl (23.2 MB)\n",
      "     ---------------------------------------- 23.2/23.2 MB 3.5 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.11.0->tensorflow) (4.3.0)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.51.3-cp39-cp39-win_amd64.whl (3.7 MB)\n",
      "     ---------------------------------------- 3.7/3.7 MB 7.2 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     ---------------------------------------- 65.5/65.5 kB 1.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.11.0->tensorflow) (0.37.1)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.1-py2.py3-none-any.whl (177 kB)\n",
      "     -------------------------------------- 177.2/177.2 kB 2.7 MB/s eta 0:00:00\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "     -------------------------------------- 781.3/781.3 kB 5.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.28.1)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.3)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Downloading tensorboard_data_server-0.6.1-py3-none-any.whl (2.4 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.11.0->tensorflow) (3.0.9)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.2.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\govindaradha\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow-intel==2.11.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "     -------------------------------------- 151.7/151.7 kB 4.6 MB/s eta 0:00:00\n",
      "Installing collected packages: tensorboard-plugin-wit, libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, opt-einsum, oauthlib, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.1.21 gast-0.4.0 google-auth-2.16.1 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 keras-2.11.0 libclang-15.0.6.1 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-3.19.6 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-intel-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b17e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80718fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing independent and dependent variables\n",
    "def prepare_data(timeseries_data, n_features):\n",
    "  X, y = [], []\n",
    "  for i in range(len(timeseries_data)):\n",
    "    # find the end of this pattern\n",
    "    end_ix = i + n_features\n",
    "    # check if we are beyond the sequence \n",
    "    if end_ix > len(timeseries_data)-1:\n",
    "      break\n",
    "    # gather input and output parts of the pattern\n",
    "    seq_x, seq_y = timeseries_data[i:end_ix], timeseries_data[end_ix]\n",
    "    X.append(seq_x)\n",
    "    y.append(seq_y)\n",
    "  return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c756eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number sequence\n",
    "timeseries_data = [100, 125, 133, 146, 158, 172, 187, 196, 210]\n",
    "# choose a number of time steps\n",
    "n_steps=3\n",
    "# split into samples\n",
    "X, y = prepare_data(timeseries_data, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01d6936a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[100 125 133]\n",
      " [125 133 146]\n",
      " [133 146 158]\n",
      " [146 158 172]\n",
      " [158 172 187]\n",
      " [172 187 196]]\n",
      "[146 158 172 187 196 210]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X), print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c3be6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape from [samples, timesteps] into [samples, timesteps, features]\n",
    "n_features = 1\n",
    "X = X.reshape((X.shape[0], X.shape[1], n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe634cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "846ef52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "1/1 [==============================] - 7s 7s/step - loss: 33132.4453\n",
      "Epoch 2/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 32613.9609\n",
      "Epoch 3/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 32082.8691\n",
      "Epoch 4/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 31558.5254\n",
      "Epoch 5/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 31062.3145\n",
      "Epoch 6/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 30601.5469\n",
      "Epoch 7/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 30163.3535\n",
      "Epoch 8/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 29733.0781\n",
      "Epoch 9/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 29291.0645\n",
      "Epoch 10/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 28810.3594\n",
      "Epoch 11/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 28272.7012\n",
      "Epoch 12/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 27654.6465\n",
      "Epoch 13/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 26916.5098\n",
      "Epoch 14/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 26016.6719\n",
      "Epoch 15/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 24967.4473\n",
      "Epoch 16/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 23857.3770\n",
      "Epoch 17/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 22807.6113\n",
      "Epoch 18/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 21869.7871\n",
      "Epoch 19/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 21010.5156\n",
      "Epoch 20/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 20211.8730\n",
      "Epoch 21/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 19424.5020\n",
      "Epoch 22/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 18612.2754\n",
      "Epoch 23/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 17733.2656\n",
      "Epoch 24/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 16743.6660\n",
      "Epoch 25/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15603.7227\n",
      "Epoch 26/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 14293.4346\n",
      "Epoch 27/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12827.8174\n",
      "Epoch 28/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11268.9502\n",
      "Epoch 29/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9701.6279\n",
      "Epoch 30/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8140.0996\n",
      "Epoch 31/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 6504.3413\n",
      "Epoch 32/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4843.7466\n",
      "Epoch 33/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3428.3340\n",
      "Epoch 34/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2299.0986\n",
      "Epoch 35/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1384.2477\n",
      "Epoch 36/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 679.3762\n",
      "Epoch 37/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 216.7588\n",
      "Epoch 38/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 57.7389\n",
      "Epoch 39/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 279.3253\n",
      "Epoch 40/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 608.3188\n",
      "Epoch 41/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 829.6182\n",
      "Epoch 42/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 847.3779\n",
      "Epoch 43/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 741.0222\n",
      "Epoch 44/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 651.3580\n",
      "Epoch 45/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 566.3337\n",
      "Epoch 46/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 462.7264\n",
      "Epoch 47/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 351.8259\n",
      "Epoch 48/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 244.2796\n",
      "Epoch 49/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 151.8781\n",
      "Epoch 50/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 84.4985\n",
      "Epoch 51/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 46.7745\n",
      "Epoch 52/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 39.1815\n",
      "Epoch 53/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 57.9391\n",
      "Epoch 54/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 84.4626\n",
      "Epoch 55/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 105.9124\n",
      "Epoch 56/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 118.3889\n",
      "Epoch 57/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 123.1294\n",
      "Epoch 58/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 121.8002\n",
      "Epoch 59/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 114.3125\n",
      "Epoch 60/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 101.0776\n",
      "Epoch 61/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 83.7724\n",
      "Epoch 62/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 65.6852\n",
      "Epoch 63/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 51.0842\n",
      "Epoch 64/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 43.1241\n",
      "Epoch 65/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 41.7826\n",
      "Epoch 66/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 44.7166\n",
      "Epoch 67/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 49.5035\n",
      "Epoch 68/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 54.1544\n",
      "Epoch 69/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 57.4291\n",
      "Epoch 70/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 58.7044\n",
      "Epoch 71/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 57.8857\n",
      "Epoch 72/300\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 55.2970\n",
      "Epoch 73/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 51.5255\n",
      "Epoch 74/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 47.2407\n",
      "Epoch 75/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 43.0138\n",
      "Epoch 76/300\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 39.1890\n",
      "Epoch 77/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 35.8230\n",
      "Epoch 78/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 32.8049\n",
      "Epoch 79/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 30.5783\n",
      "Epoch 80/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 30.6102\n",
      "Epoch 81/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 29.0216\n",
      "Epoch 82/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 29.1335\n",
      "Epoch 83/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 29.7294\n",
      "Epoch 84/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 29.3826\n",
      "Epoch 85/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 28.9682\n",
      "Epoch 86/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 29.4284\n",
      "Epoch 87/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 29.5392\n",
      "Epoch 88/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 28.8314\n",
      "Epoch 89/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 27.8908\n",
      "Epoch 90/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 26.9955\n",
      "Epoch 91/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 25.4874\n",
      "Epoch 92/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 23.1816\n",
      "Epoch 93/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 21.3050\n",
      "Epoch 94/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 20.7033\n",
      "Epoch 95/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 18.2258\n",
      "Epoch 96/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 16.1387\n",
      "Epoch 97/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 15.3148\n",
      "Epoch 98/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 14.9398\n",
      "Epoch 99/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 14.2840\n",
      "Epoch 100/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 13.5659\n",
      "Epoch 101/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 12.7898\n",
      "Epoch 102/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 12.1183\n",
      "Epoch 103/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 11.3003\n",
      "Epoch 104/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 10.5145\n",
      "Epoch 105/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.7310\n",
      "Epoch 106/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.2586\n",
      "Epoch 107/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 8.6140\n",
      "Epoch 108/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 8.1131\n",
      "Epoch 109/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.8815\n",
      "Epoch 110/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.3017\n",
      "Epoch 111/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.9820\n",
      "Epoch 112/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 6.6095\n",
      "Epoch 113/300\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.2894\n",
      "Epoch 114/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.9679\n",
      "Epoch 115/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.6985\n",
      "Epoch 116/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.5667\n",
      "Epoch 117/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.3714\n",
      "Epoch 118/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 5.2367\n",
      "Epoch 119/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.1018\n",
      "Epoch 120/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.0684\n",
      "Epoch 121/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.9118\n",
      "Epoch 122/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.8437\n",
      "Epoch 123/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.7446\n",
      "Epoch 124/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.6667\n",
      "Epoch 125/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.6319\n",
      "Epoch 126/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.5826\n",
      "Epoch 127/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.5830\n",
      "Epoch 128/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.5356\n",
      "Epoch 129/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.4815\n",
      "Epoch 130/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.4448\n",
      "Epoch 131/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.3959\n",
      "Epoch 132/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.3873\n",
      "Epoch 133/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.3587\n",
      "Epoch 134/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.3344\n",
      "Epoch 135/300\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 4.3113\n",
      "Epoch 136/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.2721\n",
      "Epoch 137/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.2509\n",
      "Epoch 138/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 4.2156\n",
      "Epoch 139/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.1929\n",
      "Epoch 140/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.1693\n",
      "Epoch 141/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.1398\n",
      "Epoch 142/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.1162\n",
      "Epoch 143/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 4.0755\n",
      "Epoch 144/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.0421\n",
      "Epoch 145/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.0243\n",
      "Epoch 146/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.0165\n",
      "Epoch 147/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.9948\n",
      "Epoch 148/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.9657\n",
      "Epoch 149/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.9336\n",
      "Epoch 150/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.9043\n",
      "Epoch 151/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.8786\n",
      "Epoch 152/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8587\n",
      "Epoch 153/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.8402\n",
      "Epoch 154/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.8176\n",
      "Epoch 155/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.7887\n",
      "Epoch 156/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.7595\n",
      "Epoch 157/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.7356\n",
      "Epoch 158/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.7160\n",
      "Epoch 159/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.6975\n",
      "Epoch 160/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.6808\n",
      "Epoch 161/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.6868\n",
      "Epoch 162/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 3.7173\n",
      "Epoch 163/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.6691\n",
      "Epoch 164/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.5838\n",
      "Epoch 165/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.5883\n",
      "Epoch 166/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.5991\n",
      "Epoch 167/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.5346\n",
      "Epoch 168/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.5086\n",
      "Epoch 169/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.5324\n",
      "Epoch 170/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.5001\n",
      "Epoch 171/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.4457\n",
      "Epoch 172/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.4585\n",
      "Epoch 173/300\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.4450\n",
      "Epoch 174/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.3918\n",
      "Epoch 175/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.3927\n",
      "Epoch 176/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.3895\n",
      "Epoch 177/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.3396\n",
      "Epoch 178/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.3257\n",
      "Epoch 179/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.3293\n",
      "Epoch 180/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.2929\n",
      "Epoch 181/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.2607\n",
      "Epoch 182/300\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.2579\n",
      "Epoch 183/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.2444\n",
      "Epoch 184/300\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.2093\n",
      "Epoch 185/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.1825\n",
      "Epoch 186/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.1746\n",
      "Epoch 187/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.1642\n",
      "Epoch 188/300\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.1369\n",
      "Epoch 189/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.1050\n",
      "Epoch 190/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.0823\n",
      "Epoch 191/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.0695\n",
      "Epoch 192/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.0605\n",
      "Epoch 193/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.0498\n",
      "Epoch 194/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.0365\n",
      "Epoch 195/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.0128\n",
      "Epoch 196/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.9832\n",
      "Epoch 197/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.9498\n",
      "Epoch 198/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.9211\n",
      "Epoch 199/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.8991\n",
      "Epoch 200/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.8827\n",
      "Epoch 201/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 2.8730\n",
      "Epoch 202/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.8787\n",
      "Epoch 203/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.9255\n",
      "Epoch 204/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.9941\n",
      "Epoch 205/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 3.0018\n",
      "Epoch 206/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.8060\n",
      "Epoch 207/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7872\n",
      "Epoch 208/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.8964\n",
      "Epoch 209/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7861\n",
      "Epoch 210/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.7187\n",
      "Epoch 211/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.7989\n",
      "Epoch 212/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7390\n",
      "Epoch 213/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.6691\n",
      "Epoch 214/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7178\n",
      "Epoch 215/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.6986\n",
      "Epoch 216/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.6297\n",
      "Epoch 217/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.6386\n",
      "Epoch 218/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6585\n",
      "Epoch 219/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.6166\n",
      "Epoch 220/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.5755\n",
      "Epoch 221/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.5905\n",
      "Epoch 222/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6029\n",
      "Epoch 223/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.5648\n",
      "Epoch 224/300\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.5277\n",
      "Epoch 225/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.5270\n",
      "Epoch 226/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.5394\n",
      "Epoch 227/300\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5339\n",
      "Epoch 228/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.5023\n",
      "Epoch 229/300\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.4732\n",
      "Epoch 230/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.4621\n",
      "Epoch 231/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4659\n",
      "Epoch 232/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4753\n",
      "Epoch 233/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.4797\n",
      "Epoch 234/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.4806\n",
      "Epoch 235/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.4656\n",
      "Epoch 236/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4446\n",
      "Epoch 237/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4161\n",
      "Epoch 238/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.3926\n",
      "Epoch 239/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3764\n",
      "Epoch 240/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3676\n",
      "Epoch 241/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.3645\n",
      "Epoch 242/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.3690\n",
      "Epoch 243/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3912\n",
      "Epoch 244/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.4506\n",
      "Epoch 245/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5967\n",
      "Epoch 246/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7101\n",
      "Epoch 247/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6524\n",
      "Epoch 248/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.3511\n",
      "Epoch 249/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3835\n",
      "Epoch 250/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.5761\n",
      "Epoch 251/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.4122\n",
      "Epoch 252/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3043\n",
      "Epoch 253/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.4467\n",
      "Epoch 254/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3946\n",
      "Epoch 255/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2861\n",
      "Epoch 256/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.3689\n",
      "Epoch 257/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.3632\n",
      "Epoch 258/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2769\n",
      "Epoch 259/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.3191\n",
      "Epoch 260/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3353\n",
      "Epoch 261/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2706\n",
      "Epoch 262/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2819\n",
      "Epoch 263/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3104\n",
      "Epoch 264/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2684\n",
      "Epoch 265/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2535\n",
      "Epoch 266/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2834\n",
      "Epoch 267/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2692\n",
      "Epoch 268/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.2372\n",
      "Epoch 269/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2506\n",
      "Epoch 270/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2627\n",
      "Epoch 271/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2370\n",
      "Epoch 272/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2227\n",
      "Epoch 273/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2359\n",
      "Epoch 274/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2380\n",
      "Epoch 275/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2197\n",
      "Epoch 276/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2077\n",
      "Epoch 277/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2137\n",
      "Epoch 278/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.2193\n",
      "Epoch 279/300\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2099\n",
      "Epoch 280/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.1960\n",
      "Epoch 281/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.1903\n",
      "Epoch 282/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.1922\n",
      "Epoch 283/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.1932\n",
      "Epoch 284/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.1805\n",
      "Epoch 285/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.1668\n",
      "Epoch 286/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.1497\n",
      "Epoch 287/300\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.1601\n",
      "Epoch 288/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3710\n",
      "Epoch 289/300\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.0615\n",
      "Epoch 290/300\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.3949\n",
      "Epoch 291/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5781\n",
      "Epoch 292/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.2599\n",
      "Epoch 293/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.9598\n",
      "Epoch 294/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3529\n",
      "Epoch 295/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3755\n",
      "Epoch 296/300\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6408\n",
      "Epoch 297/300\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.1340\n",
      "Epoch 298/300\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.5159\n",
      "Epoch 299/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2134\n",
      "Epoch 300/300\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3093\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x164efe2c160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='relu', return_sequences = True, input_shape = (n_steps, n_features)))\n",
    "model.add(LSTM(50, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "#fit model\n",
    "model.fit(X,y, epochs = 300, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98b4e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[217.35133]\n",
      "1 day input [196.         210.         217.35133362]\n",
      "1 day output [[228.26642]]\n",
      "2 day input [210.         217.35133362 228.26641846]\n",
      "2 day output [[235.84247]]\n",
      "3 day input [217.35133 228.26642 235.84247]\n",
      "3 day output [[244.75388]]\n",
      "4 day input [228.26642 235.84247 244.75388]\n",
      "4 day output [[252.77475]]\n",
      "5 day input [235.84247 244.75388 252.77475]\n",
      "5 day output [[260.78766]]\n",
      "6 day input [244.75388 252.77475 260.78766]\n",
      "6 day output [[268.79507]]\n",
      "7 day input [252.77475 260.78766 268.79507]\n",
      "7 day output [[276.61554]]\n",
      "8 day input [260.78766 268.79507 276.61554]\n",
      "8 day output [[284.4669]]\n",
      "9 day input [268.79507 276.61554 284.4669 ]\n",
      "9 day output [[292.29343]]\n"
     ]
    }
   ],
   "source": [
    "# demonstrate prediction for next 10 days\n",
    "x_input = np.array([187, 196, 210])\n",
    "temp_input=list(x_input)\n",
    "lst_output=[]\n",
    "i=0\n",
    "while(i<10):\n",
    "    \n",
    "    if(len(temp_input)>3):\n",
    "        x_input=np.array(temp_input[1:])\n",
    "        print(\"{} day input {}\".format(i,x_input))\n",
    "        #print(x_input)\n",
    "        x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        #print(x_input)\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(\"{} day output {}\".format(i,yhat))\n",
    "        temp_input.append(yhat[0][0])\n",
    "        temp_input=temp_input[1:]\n",
    "        #print(temp_input)\n",
    "        lst_output.append(yhat[0][0])\n",
    "        i=i+1\n",
    "    else:\n",
    "        x_input = x_input.reshape((1, n_steps, n_features))\n",
    "        yhat = model.predict(x_input, verbose=0)\n",
    "        print(yhat[0])\n",
    "        temp_input.append(yhat[0][0])\n",
    "        lst_output.append(yhat[0][0])\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af37770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
